{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dbc72a5-21f5-487b-8171-65dfb017d4e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, RobustScaler\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.inspection import permutation_importance\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Load dataset\n",
    "# Note: You'll need to replace this path with the location of your earthquake dataset\n",
    "df = pd.read_csv(r\"C:\\Users\\jidub\\OneDrive\\Documents\\Timeseries\\project\\Eartquakes-1990-2023.csv\\Eartquakes-1990-2023.csv\")\n",
    "\n",
    "# Preprocess timestamp\n",
    "df['time'] = pd.to_datetime(df['time'], unit='ms')\n",
    "df = df.sort_values('time')  # Ensure data is chronologically ordered\n",
    "df['year'] = df['time'].dt.year\n",
    "df['month'] = df['time'].dt.month\n",
    "df['day'] = df['time'].dt.day\n",
    "df['hour'] = df['time'].dt.hour\n",
    "\n",
    "# Handle missing values\n",
    "df.fillna(method='ffill', inplace=True)\n",
    "\n",
    "# === Feature Engineering ===\n",
    "# 1. Add cyclical time features\n",
    "df['month_sin'] = np.sin(df['month'] * (2 * np.pi / 12))\n",
    "df['month_cos'] = np.cos(df['month'] * (2 * np.pi / 12))\n",
    "df['hour_sin'] = np.sin(df['hour'] * (2 * np.pi / 24))\n",
    "df['hour_cos'] = np.cos(df['hour'] * (2 * np.pi / 24))\n",
    "df['day_sin'] = np.sin(df['day'] * (2 * np.pi / 31))\n",
    "df['day_cos'] = np.cos(df['day'] * (2 * np.pi / 31))\n",
    "\n",
    "# 2. Rolling statistics\n",
    "for window in [7, 30, 90]:\n",
    "    df[f'rolling_mag_mean_{window}d'] = df['magnitudo'].rolling(window=window).mean()\n",
    "    df[f'rolling_depth_mean_{window}d'] = df['depth'].rolling(window=window).mean()\n",
    "    df[f'rolling_quake_count_{window}d'] = df['magnitudo'].rolling(window=window).count()\n",
    "\n",
    "# 3. Handle outliers\n",
    "def cap_outliers(series, lower_quantile=0.01, upper_quantile=0.99):\n",
    "    lower_bound = series.quantile(lower_quantile)\n",
    "    upper_bound = series.quantile(upper_quantile)\n",
    "    return series.clip(lower=lower_bound, upper=upper_bound)\n",
    "\n",
    "df['magnitudo'] = cap_outliers(df['magnitudo'])\n",
    "df['depth'] = cap_outliers(df['depth'])\n",
    "\n",
    "# 4. Create interaction features\n",
    "df['lat_lon_interaction'] = df['latitude'] * df['longitude'] / 10000  # Scaled down\n",
    "\n",
    "# 5. Create magnitude categories\n",
    "df['mag_low'] = (df['magnitudo'] < 2.0).astype(int)\n",
    "df['mag_medium'] = ((df['magnitudo'] >= 2.0) & (df['magnitudo'] < 4.0)).astype(int)\n",
    "df['mag_high'] = (df['magnitudo'] >= 4.0).astype(int)\n",
    "\n",
    "# 6. Add geographic region clustering\n",
    "def assign_region(lat, lon):\n",
    "    # Simplified region assignment based on latitude/longitude\n",
    "    if lat > 30 and lon > 120:  # Pacific Ring of Fire - Asia side\n",
    "        return 1\n",
    "    elif lat > 30 and lon < -100:  # Pacific Ring of Fire - Americas side\n",
    "        return 2\n",
    "    elif -30 < lat < 30:  # Equatorial regions\n",
    "        return 3\n",
    "    else:  # Other regions\n",
    "        return 4\n",
    "\n",
    "df['region'] = df.apply(lambda x: assign_region(x['latitude'], x['longitude']), axis=1)\n",
    "df = pd.get_dummies(df, columns=['region'], prefix='region')\n",
    "\n",
    "# Fill any remaining NaN values\n",
    "df.fillna(method='bfill', inplace=True)\n",
    "\n",
    "# Define features\n",
    "features = [\n",
    "    'latitude', 'longitude', 'depth',  \n",
    "    'month_sin', 'month_cos', 'day_sin', 'day_cos', 'hour_sin', 'hour_cos',\n",
    "    'rolling_mag_mean_30d', 'rolling_depth_mean_30d', 'rolling_quake_count_30d',\n",
    "    'rolling_mag_mean_7d', 'rolling_mag_mean_90d',\n",
    "    'lat_lon_interaction', 'mag_low', 'mag_medium', 'mag_high',\n",
    "    'region_1', 'region_2', 'region_3', 'region_4'\n",
    "]\n",
    "\n",
    "targets = ['latitude', 'longitude', 'magnitudo']\n",
    "\n",
    "# Extract features and targets\n",
    "X = df[features].values\n",
    "y = df[targets].values\n",
    "\n",
    "# Use time-based split for temporal data\n",
    "test_size = int(0.1 * len(X))\n",
    "val_size = int(0.1 * len(X))\n",
    "\n",
    "X_train = X[:-test_size-val_size]\n",
    "X_val = X[-test_size-val_size:-test_size]\n",
    "X_test = X[-test_size:]\n",
    "\n",
    "y_train = y[:-test_size-val_size]\n",
    "y_val = y[-test_size-val_size:-test_size]\n",
    "y_test = y[-test_size:]\n",
    "\n",
    "# Scale the features\n",
    "feature_scaler = RobustScaler()\n",
    "X_train_scaled = feature_scaler.fit_transform(X_train)\n",
    "X_val_scaled = feature_scaler.transform(X_val)\n",
    "X_test_scaled = feature_scaler.transform(X_test)\n",
    "\n",
    "# Random Forest doesn't require target scaling, but we'll scale for evaluation consistency\n",
    "target_scaler = StandardScaler()\n",
    "y_train_scaled = target_scaler.fit_transform(y_train)\n",
    "y_val_scaled = target_scaler.transform(y_val)\n",
    "y_test_scaled = target_scaler.transform(y_test)\n",
    "\n",
    "# Train a separate Random Forest model for each target\n",
    "models = []\n",
    "target_names = ['Latitude', 'Longitude', 'Magnitude']\n",
    "\n",
    "for i in range(3):\n",
    "    print(f\"\\nTraining Random Forest for {target_names[i]}...\")\n",
    "    \n",
    "    # Create and train the model\n",
    "    rf_model = RandomForestRegressor(\n",
    "        n_estimators=100,        # Number of trees\n",
    "        max_depth=15,            # Maximum depth of trees\n",
    "        min_samples_split=5,     # Minimum samples required to split\n",
    "        min_samples_leaf=2,      # Minimum samples required at leaf node\n",
    "        max_features='sqrt',     # Max features to consider for best split\n",
    "        n_jobs=-1,               # Use all available processors\n",
    "        random_state=42          # For reproducibility\n",
    "    )\n",
    "    \n",
    "    # Fit the model to the training data\n",
    "    rf_model.fit(X_train_scaled, y_train_scaled[:, i])\n",
    "    models.append(rf_model)\n",
    "    \n",
    "    # Make predictions on validation set to check performance\n",
    "    val_pred = rf_model.predict(X_val_scaled)\n",
    "    val_mse = mean_squared_error(y_val_scaled[:, i], val_pred)\n",
    "    val_rmse = np.sqrt(val_mse)\n",
    "    val_mae = mean_absolute_error(y_val_scaled[:, i], val_pred)\n",
    "    val_r2 = r2_score(y_val_scaled[:, i], val_pred)\n",
    "    \n",
    "    print(f\"Validation RMSE: {val_rmse:.4f}\")\n",
    "    print(f\"Validation MAE: {val_mae:.4f}\")\n",
    "    print(f\"Validation R²: {val_r2:.4f}\")\n",
    "\n",
    "# Evaluate models on test set\n",
    "print(\"\\n=== Random Forest Model Performance ===\")\n",
    "\n",
    "for i, (model, target_name) in enumerate(zip(models, target_names)):\n",
    "    # Make predictions\n",
    "    y_train_pred = model.predict(X_train_scaled)\n",
    "    y_test_pred = model.predict(X_test_scaled)\n",
    "    \n",
    "    # Convert predictions to original scale for a single target\n",
    "    y_train_true_single = y_train_scaled[:, i]\n",
    "    y_test_true_single = y_test_scaled[:, i]\n",
    "    \n",
    "    # Calculate metrics\n",
    "    train_mse = mean_squared_error(y_train_true_single, y_train_pred)\n",
    "    test_mse = mean_squared_error(y_test_true_single, y_test_pred)\n",
    "    train_rmse = np.sqrt(train_mse)\n",
    "    test_rmse = np.sqrt(test_mse)\n",
    "    train_mae = mean_absolute_error(y_train_true_single, y_train_pred)\n",
    "    test_mae = mean_absolute_error(y_test_true_single, y_test_pred)\n",
    "    train_r2 = r2_score(y_train_true_single, y_train_pred)\n",
    "    test_r2 = r2_score(y_test_true_single, y_test_pred)\n",
    "    \n",
    "    # Overfitting ratio\n",
    "    mse_overfitting_ratio = test_mse / train_mse if train_mse > 0 else float('inf')\n",
    "    \n",
    "    print(f\"\\n{target_name} Performance:\")\n",
    "    print(f\"  Train MSE: {train_mse:.4f}, Test MSE: {test_mse:.4f}\")\n",
    "    print(f\"  Train RMSE: {train_rmse:.4f}, Test RMSE: {test_rmse:.4f}\")\n",
    "    print(f\"  Train MAE: {train_mae:.4f}, Test MAE: {test_mae:.4f}\")\n",
    "    print(f\"  Train R²: {train_r2:.4f}, Test R²: {test_r2:.4f}\")\n",
    "    print(f\"  MSE Overfitting Ratio (Test/Train): {mse_overfitting_ratio:.2f}\")\n",
    "\n",
    "# Visualize feature importances\n",
    "plt.figure(figsize=(14, 10))\n",
    "\n",
    "for i, (model, target_name) in enumerate(zip(models, target_names)):\n",
    "    plt.subplot(3, 1, i+1)\n",
    "    \n",
    "    # Get feature importances\n",
    "    importances = model.feature_importances_\n",
    "    indices = np.argsort(importances)\n",
    "    \n",
    "    plt.barh(range(len(indices)), importances[indices], color='skyblue')\n",
    "    plt.yticks(range(len(indices)), [features[i] for i in indices])\n",
    "    plt.title(f'Feature Importance for {target_name}')\n",
    "    plt.xlabel('Relative Importance')\n",
    "    plt.tight_layout()\n",
    "\n",
    "plt.savefig('random_forest_feature_importance.png')\n",
    "plt.close()\n",
    "\n",
    "# Visualize predictions vs actual values\n",
    "plt.figure(figsize=(15, 15))\n",
    "\n",
    "# Use a subset of test data for clearer visualization\n",
    "test_indices = np.random.choice(range(len(y_test_scaled)), min(100, len(y_test_scaled)), replace=False)\n",
    "\n",
    "for i, (model, target_name) in enumerate(zip(models, target_names)):\n",
    "    plt.subplot(3, 1, i+1)\n",
    "    \n",
    "    # Make predictions\n",
    "    y_pred = model.predict(X_test_scaled)\n",
    "    \n",
    "    # Plot\n",
    "    plt.plot(y_test_scaled[test_indices, i], label='Actual', marker='o', linestyle='', alpha=0.7)\n",
    "    plt.plot(y_pred[test_indices], label='Predicted', marker='x', linestyle='', alpha=0.7)\n",
    "    plt.title(f'{target_name} - Actual vs Predicted')\n",
    "    plt.xlabel('Sample Index')\n",
    "    plt.ylabel(target_name)\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('random_forest_predictions.png')\n",
    "plt.close()\n",
    "\n",
    "# Visualize error distribution\n",
    "plt.figure(figsize=(15, 5))\n",
    "\n",
    "for i, (model, target_name) in enumerate(zip(models, target_names)):\n",
    "    plt.subplot(1, 3, i+1)\n",
    "    \n",
    "    # Make predictions\n",
    "    y_pred = model.predict(X_test_scaled)\n",
    "    \n",
    "    # Calculate errors\n",
    "    errors = y_test_scaled[:, i] - y_pred\n",
    "    \n",
    "    # Plot error distribution\n",
    "    plt.hist(errors, bins=30, color=['skyblue', 'lightgreen', 'salmon'][i], edgecolor='black')\n",
    "    plt.title(f'{target_name} Error Distribution')\n",
    "    plt.xlabel('Error')\n",
    "    plt.axvline(x=0, color='red', linestyle='--')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('random_forest_error_distribution.png')\n",
    "plt.close()\n",
    "\n",
    "# Function to make predictions with all three models\n",
    "def predict_with_random_forest(models, input_data, feature_scaler, target_scaler):\n",
    "    \"\"\"\n",
    "    Make predictions using all three random forest models\n",
    "    \n",
    "    Parameters:\n",
    "    models: List of trained Random Forest models [lat_model, lon_model, mag_model]\n",
    "    input_data: Numpy array of shape (n_samples, n_features)\n",
    "    feature_scaler: Fitted scaler for input features\n",
    "    target_scaler: Fitted scaler for target variables\n",
    "    \n",
    "    Returns:\n",
    "    prediction: Dictionary with predicted latitude, longitude, and magnitude\n",
    "    \"\"\"\n",
    "    # Scale the input data\n",
    "    scaled_input = feature_scaler.transform(input_data)\n",
    "    \n",
    "    # Get predictions from each model\n",
    "    lat_pred = models[0].predict(scaled_input)\n",
    "    lon_pred = models[1].predict(scaled_input)\n",
    "    mag_pred = models[2].predict(scaled_input)\n",
    "    \n",
    "    # Combine predictions\n",
    "    combined_pred = np.column_stack((lat_pred, lon_pred, mag_pred))\n",
    "    \n",
    "    # If you need to convert back to original scale:\n",
    "    # original_pred = target_scaler.inverse_transform(combined_pred)\n",
    "    \n",
    "    return {\n",
    "        'latitude': lat_pred,\n",
    "        'longitude': lon_pred,\n",
    "        'magnitude': mag_pred\n",
    "    }\n",
    "\n",
    "print(\"\\nRandom Forest models trained successfully. Use the predict_with_random_forest() function to make new predictions.\")\n",
    "\n",
    "# Compare Random Forest with Deep Learning performance\n",
    "print(\"\\n=== Random Forest vs. Deep Learning Performance Comparison ===\")\n",
    "print(\"Note: The deep learning model results need to be manually compared from the previous output.\")\n",
    "print(\"General advantages of Random Forest for this dataset:\")\n",
    "print(\"1. Faster training time\")\n",
    "print(\"2. Less prone to overfitting with proper tuning\")\n",
    "print(\"3. Better interpretability through feature importance\")\n",
    "print(\"4. Can handle non-linear relationships without extensive feature engineering\")\n",
    "print(\"5. No need for sequence processing or time window preparation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d36ccea0-a9c2-4805-9261-97887b9069ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pywt\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
    "\n",
    "# Load dataset\n",
    "df = pd.read_csv(r\"C:\\Users\\jidub\\OneDrive\\Documents\\Timeseries\\project\\Eartquakes-1990-2023.csv\\Eartquakes-1990-2023.csv\")\n",
    "df['date'] = pd.to_datetime(df['date'], errors='coerce')\n",
    "df.set_index('date', inplace=True)\n",
    "\n",
    "data = df['magnitudo'].resample('M').mean().dropna()\n",
    "\n",
    "# Normalize data\n",
    "scaler = MinMaxScaler()\n",
    "data_scaled = scaler.fit_transform(data.values.reshape(-1, 1)).flatten()\n",
    "\n",
    "# Apply Discrete Wavelet Transform (DWT)\n",
    "wavename = 'db4'  # Daubechies wavelet\n",
    "coeffs = pywt.wavedec(data_scaled, wavename, level=3)\n",
    "cA3, cD3, cD2, cD1 = coeffs  # Approximation and details\n",
    "\n",
    "# Reconstruct smoothed signal\n",
    "smoothed_data = pywt.waverec([cA3] + [None] * 3, wavename)[:len(data_scaled)]\n",
    "\n",
    "# Prepare sequences for LSTM\n",
    "sequence_length = 30\n",
    "X, y = [], []\n",
    "for i in range(len(smoothed_data) - sequence_length):\n",
    "    X.append(smoothed_data[i:i+sequence_length])\n",
    "    y.append(smoothed_data[i+sequence_length])\n",
    "X, y = np.array(X), np.array(y)\n",
    "\n",
    "# Reshape input for LSTM\n",
    "X = X.reshape(X.shape[0], X.shape[1], 1)\n",
    "\n",
    "# Split into train and test sets\n",
    "split = int(0.8 * len(X))\n",
    "X_train, X_test = X[:split], X[split:]\n",
    "y_train, y_test = y[:split], y[split:]\n",
    "\n",
    "# Build LSTM model\n",
    "model = Sequential([\n",
    "    LSTM(100, return_sequences=True, input_shape=(sequence_length, 1)),\n",
    "    Dropout(0.3),\n",
    "    LSTM(100, return_sequences=False),\n",
    "    Dropout(0.3),\n",
    "    Dense(1)\n",
    "])\n",
    "\n",
    "# Compile and train model\n",
    "model.compile(optimizer='adam', loss='mse')\n",
    "model.fit(X_train, y_train, epochs=100, batch_size=32, validation_data=(X_test, y_test))\n",
    "\n",
    "# Forecasting\n",
    "lstm_forecast = model.predict(X_test[-30:])\n",
    "lstm_forecast = scaler.inverse_transform(lstm_forecast.reshape(-1, 1))\n",
    "\n",
    "# Evaluate model\n",
    "y_test_actual = scaler.inverse_transform(y_test.reshape(-1, 1))\n",
    "y_test_predicted = scaler.inverse_transform(model.predict(X_test))\n",
    "\n",
    "mae = mean_absolute_error(y_test_actual, y_test_predicted)\n",
    "rmse = np.sqrt(mean_squared_error(y_test_actual, y_test_predicted))\n",
    "mape = np.mean(np.abs((y_test_actual - y_test_predicted) / y_test_actual)) * 100\n",
    "accuracy = 100 - mape  # Accuracy as (100 - MAPE)\n",
    "\n",
    "# Print results\n",
    "print(f\"Hybrid LSTM-Wavelet Model Accuracy:\")\n",
    "print(f\"Mean Absolute Error (MAE): {mae:.4f}\")\n",
    "print(f\"Root Mean Squared Error (RMSE): {rmse:.4f}\")\n",
    "print(f\"Mean Absolute Percentage Error (MAPE): {mape:.2f}%\")\n",
    "print(f\"Forecast Accuracy: {accuracy:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1fdb984-827c-4afd-9880-a59a082d5f43",
   "metadata": {},
   "outputs": [],
   "source": [
    "Combined Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "96f12254-cd95-46a0-9065-ccb390ece389",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jidub\\AppData\\Local\\Temp\\ipykernel_22840\\223186086.py:31: FutureWarning: DataFrame.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  df.fillna(method='ffill', inplace=True)\n",
      "C:\\Users\\jidub\\AppData\\Local\\Temp\\ipykernel_22840\\223186086.py:81: FutureWarning: DataFrame.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  df.fillna(method='bfill', inplace=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Random Forest for Latitude...\n",
      "Validation RMSE: 0.0257\n",
      "Validation MAE: 0.0137\n",
      "Validation R²: 0.9992\n",
      "\n",
      "Training Random Forest for Longitude...\n",
      "Validation RMSE: 0.0194\n",
      "Validation MAE: 0.0100\n",
      "Validation R²: 0.9994\n",
      "\n",
      "Training LSTM-Wavelet Model for Magnitude...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jidub\\AppData\\Local\\Temp\\ipykernel_22840\\223186086.py:164: FutureWarning: 'M' is deprecated and will be removed in a future version, please use 'ME' instead.\n",
      "  monthly_mag = mag_data['magnitudo'].resample('M').mean().dropna()\n",
      "C:\\Users\\jidub\\anaconda3\\Lib\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 126ms/step - loss: 0.1557 - val_loss: 0.0051\n",
      "Epoch 2/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 58ms/step - loss: 0.0249 - val_loss: 0.0144\n",
      "Epoch 3/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 51ms/step - loss: 0.0175 - val_loss: 0.0050\n",
      "Epoch 4/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 56ms/step - loss: 0.0137 - val_loss: 0.0111\n",
      "Epoch 5/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 52ms/step - loss: 0.0145 - val_loss: 0.0060\n",
      "Epoch 6/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step - loss: 0.0108 - val_loss: 0.0069\n",
      "Epoch 7/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step - loss: 0.0114 - val_loss: 0.0077\n",
      "Epoch 8/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 53ms/step - loss: 0.0111 - val_loss: 0.0053\n",
      "Epoch 9/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - loss: 0.0097 - val_loss: 0.0048\n",
      "Epoch 10/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step - loss: 0.0099 - val_loss: 0.0052\n",
      "Epoch 11/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - loss: 0.0094 - val_loss: 0.0046\n",
      "Epoch 12/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - loss: 0.0097 - val_loss: 0.0046\n",
      "Epoch 13/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step - loss: 0.0092 - val_loss: 0.0047\n",
      "Epoch 14/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step - loss: 0.0092 - val_loss: 0.0053\n",
      "Epoch 15/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step - loss: 0.0088 - val_loss: 0.0047\n",
      "Epoch 16/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - loss: 0.0102 - val_loss: 0.0043\n",
      "Epoch 17/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step - loss: 0.0088 - val_loss: 0.0040\n",
      "Epoch 18/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step - loss: 0.0085 - val_loss: 0.0037\n",
      "Epoch 19/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 51ms/step - loss: 0.0076 - val_loss: 0.0039\n",
      "Epoch 20/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step - loss: 0.0084 - val_loss: 0.0038\n",
      "Epoch 21/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step - loss: 0.0078 - val_loss: 0.0039\n",
      "Epoch 22/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 56ms/step - loss: 0.0079 - val_loss: 0.0034\n",
      "Epoch 23/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step - loss: 0.0076 - val_loss: 0.0046\n",
      "Epoch 24/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 56ms/step - loss: 0.0078 - val_loss: 0.0053\n",
      "Epoch 25/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 55ms/step - loss: 0.0096 - val_loss: 0.0037\n",
      "Epoch 26/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 52ms/step - loss: 0.0083 - val_loss: 0.0032\n",
      "Epoch 27/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 49ms/step - loss: 0.0064 - val_loss: 0.0030\n",
      "Epoch 28/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 60ms/step - loss: 0.0071 - val_loss: 0.0031\n",
      "Epoch 29/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step - loss: 0.0069 - val_loss: 0.0027\n",
      "Epoch 30/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step - loss: 0.0048 - val_loss: 0.0032\n",
      "Epoch 31/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step - loss: 0.0059 - val_loss: 0.0026\n",
      "Epoch 32/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step - loss: 0.0052 - val_loss: 0.0028\n",
      "Epoch 33/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step - loss: 0.0055 - val_loss: 0.0034\n",
      "Epoch 34/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step - loss: 0.0072 - val_loss: 0.0028\n",
      "Epoch 35/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step - loss: 0.0056 - val_loss: 0.0023\n",
      "Epoch 36/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step - loss: 0.0058 - val_loss: 0.0022\n",
      "Epoch 37/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step - loss: 0.0055 - val_loss: 0.0021\n",
      "Epoch 38/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step - loss: 0.0049 - val_loss: 0.0021\n",
      "Epoch 39/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step - loss: 0.0045 - val_loss: 0.0018\n",
      "Epoch 40/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step - loss: 0.0049 - val_loss: 0.0020\n",
      "Epoch 41/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step - loss: 0.0053 - val_loss: 0.0022\n",
      "Epoch 42/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 50ms/step - loss: 0.0043 - val_loss: 0.0021\n",
      "Epoch 43/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step - loss: 0.0044 - val_loss: 0.0017\n",
      "Epoch 44/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step - loss: 0.0043 - val_loss: 0.0017\n",
      "Epoch 45/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 50ms/step - loss: 0.0044 - val_loss: 0.0018\n",
      "Epoch 46/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 50ms/step - loss: 0.0041 - val_loss: 0.0017\n",
      "Epoch 47/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 49ms/step - loss: 0.0045 - val_loss: 0.0016\n",
      "Epoch 48/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step - loss: 0.0037 - val_loss: 0.0015\n",
      "Epoch 49/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step - loss: 0.0042 - val_loss: 0.0018\n",
      "Epoch 50/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step - loss: 0.0037 - val_loss: 0.0018\n",
      "Epoch 51/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 48ms/step - loss: 0.0042 - val_loss: 0.0015\n",
      "Epoch 52/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 51ms/step - loss: 0.0047 - val_loss: 0.0016\n",
      "Epoch 53/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 52ms/step - loss: 0.0035 - val_loss: 0.0015\n",
      "Epoch 54/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 49ms/step - loss: 0.0039 - val_loss: 0.0017\n",
      "Epoch 55/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step - loss: 0.0040 - val_loss: 0.0019\n",
      "Epoch 56/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 49ms/step - loss: 0.0032 - val_loss: 0.0017\n",
      "Epoch 57/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step - loss: 0.0041 - val_loss: 0.0020\n",
      "Epoch 58/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 51ms/step - loss: 0.0039 - val_loss: 0.0024\n",
      "Epoch 59/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - loss: 0.0047 - val_loss: 0.0014\n",
      "Epoch 60/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step - loss: 0.0037 - val_loss: 0.0017\n",
      "Epoch 61/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step - loss: 0.0032 - val_loss: 0.0015\n",
      "Epoch 62/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - loss: 0.0037 - val_loss: 0.0012\n",
      "Epoch 63/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step - loss: 0.0032 - val_loss: 0.0012\n",
      "Epoch 64/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step - loss: 0.0031 - val_loss: 0.0013\n",
      "Epoch 65/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 56ms/step - loss: 0.0044 - val_loss: 0.0017\n",
      "Epoch 66/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 68ms/step - loss: 0.0041 - val_loss: 0.0015\n",
      "Epoch 67/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 60ms/step - loss: 0.0034 - val_loss: 0.0013\n",
      "Epoch 68/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 52ms/step - loss: 0.0038 - val_loss: 0.0013\n",
      "Epoch 69/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 62ms/step - loss: 0.0036 - val_loss: 0.0014\n",
      "Epoch 70/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 62ms/step - loss: 0.0028 - val_loss: 0.0011\n",
      "Epoch 71/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 47ms/step - loss: 0.0031 - val_loss: 0.0016\n",
      "Epoch 72/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step - loss: 0.0038 - val_loss: 0.0012\n",
      "Epoch 73/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step - loss: 0.0034 - val_loss: 0.0014\n",
      "Epoch 74/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step - loss: 0.0029 - val_loss: 0.0011\n",
      "Epoch 75/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step - loss: 0.0028 - val_loss: 0.0014\n",
      "Epoch 76/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step - loss: 0.0033 - val_loss: 0.0014\n",
      "Epoch 77/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 49ms/step - loss: 0.0038 - val_loss: 0.0012\n",
      "Epoch 78/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 60ms/step - loss: 0.0030 - val_loss: 0.0010\n",
      "Epoch 79/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 59ms/step - loss: 0.0024 - val_loss: 0.0010\n",
      "Epoch 80/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 68ms/step - loss: 0.0029 - val_loss: 0.0015\n",
      "Epoch 81/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 56ms/step - loss: 0.0024 - val_loss: 0.0012\n",
      "Epoch 82/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 49ms/step - loss: 0.0027 - val_loss: 0.0010\n",
      "Epoch 83/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 63ms/step - loss: 0.0032 - val_loss: 9.8557e-04\n",
      "Epoch 84/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 59ms/step - loss: 0.0024 - val_loss: 0.0011\n",
      "Epoch 85/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step - loss: 0.0033 - val_loss: 0.0010\n",
      "Epoch 86/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step - loss: 0.0034 - val_loss: 0.0011\n",
      "Epoch 87/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step - loss: 0.0031 - val_loss: 0.0011\n",
      "Epoch 88/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step - loss: 0.0025 - val_loss: 0.0015\n",
      "Epoch 89/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step - loss: 0.0033 - val_loss: 0.0011\n",
      "Epoch 90/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step - loss: 0.0030 - val_loss: 0.0012\n",
      "Epoch 91/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step - loss: 0.0031 - val_loss: 9.7940e-04\n",
      "Epoch 92/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step - loss: 0.0028 - val_loss: 0.0010\n",
      "Epoch 93/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 49ms/step - loss: 0.0029 - val_loss: 8.9595e-04\n",
      "Epoch 94/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 54ms/step - loss: 0.0027 - val_loss: 8.5186e-04\n",
      "Epoch 95/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step - loss: 0.0027 - val_loss: 8.9767e-04\n",
      "Epoch 96/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 52ms/step - loss: 0.0025 - val_loss: 8.8880e-04\n",
      "Epoch 97/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 51ms/step - loss: 0.0022 - val_loss: 9.7386e-04\n",
      "Epoch 98/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step - loss: 0.0020 - val_loss: 0.0011\n",
      "Epoch 99/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 48ms/step - loss: 0.0029 - val_loss: 0.0011\n",
      "Epoch 100/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step - loss: 0.0025 - val_loss: 8.8267e-04\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 235ms/step\n",
      "\n",
      "=== LSTM-Wavelet Magnitude Model Performance ===\n",
      "Mean Absolute Error (MAE): 0.0201\n",
      "Root Mean Squared Error (RMSE): 0.0296\n",
      "Mean Absolute Percentage Error (MAPE): 1.27%\n",
      "Forecast Accuracy: 98.73%\n",
      "\n",
      "=== Random Forest Models Performance (Latitude & Longitude) ===\n",
      "\n",
      "Latitude Performance:\n",
      "  Train MSE: 0.0009, Test MSE: 0.0009\n",
      "  Train RMSE: 0.0293, Test RMSE: 0.0292\n",
      "  Train MAE: 0.0137, Test MAE: 0.0150\n",
      "  Train R²: 0.9991, Test R²: 0.9992\n",
      "  MSE Overfitting Ratio (Test/Train): 0.99\n",
      "\n",
      "Longitude Performance:\n",
      "  Train MSE: 0.0004, Test MSE: 0.0005\n",
      "  Train RMSE: 0.0195, Test RMSE: 0.0228\n",
      "  Train MAE: 0.0094, Test MAE: 0.0116\n",
      "  Train R²: 0.9996, Test R²: 0.9993\n",
      "  MSE Overfitting Ratio (Test/Train): 1.37\n",
      "\n",
      "Combined earthquake prediction model trained successfully.\n",
      "Use the predict_earthquake() function to make new predictions.\n",
      "\n",
      "=== Model Performance Summary ===\n",
      "Latitude & Longitude: Random Forest model\n",
      "Magnitude: LSTM model with Wavelet transform\n",
      "\n",
      "The combined approach addresses overfitting in magnitude prediction\n",
      "while maintaining high accuracy for location prediction.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, RobustScaler, MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.inspection import permutation_importance\n",
    "import pywt\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "# Load dataset\n",
    "# Note: You'll need to replace this path with the location of your earthquake dataset\n",
    "df = pd.read_csv(r\"C:\\Users\\jidub\\OneDrive\\Documents\\Timeseries\\project\\Eartquakes-1990-2023.csv\\Eartquakes-1990-2023.csv\")\n",
    "\n",
    "# Preprocess timestamp\n",
    "df['time'] = pd.to_datetime(df['time'], unit='ms')\n",
    "df = df.sort_values('time')  # Ensure data is chronologically ordered\n",
    "df['year'] = df['time'].dt.year\n",
    "df['month'] = df['time'].dt.month\n",
    "df['day'] = df['time'].dt.day\n",
    "df['hour'] = df['time'].dt.hour\n",
    "\n",
    "# Handle missing values\n",
    "df.fillna(method='ffill', inplace=True)\n",
    "\n",
    "# === Feature Engineering ===\n",
    "# 1. Add cyclical time features\n",
    "df['month_sin'] = np.sin(df['month'] * (2 * np.pi / 12))\n",
    "df['month_cos'] = np.cos(df['month'] * (2 * np.pi / 12))\n",
    "df['hour_sin'] = np.sin(df['hour'] * (2 * np.pi / 24))\n",
    "df['hour_cos'] = np.cos(df['hour'] * (2 * np.pi / 24))\n",
    "df['day_sin'] = np.sin(df['day'] * (2 * np.pi / 31))\n",
    "df['day_cos'] = np.cos(df['day'] * (2 * np.pi / 31))\n",
    "\n",
    "# 2. Rolling statistics\n",
    "for window in [7, 30, 90]:\n",
    "    df[f'rolling_mag_mean_{window}d'] = df['magnitudo'].rolling(window=window).mean()\n",
    "    df[f'rolling_depth_mean_{window}d'] = df['depth'].rolling(window=window).mean()\n",
    "    df[f'rolling_quake_count_{window}d'] = df['magnitudo'].rolling(window=window).count()\n",
    "\n",
    "# 3. Handle outliers\n",
    "def cap_outliers(series, lower_quantile=0.01, upper_quantile=0.99):\n",
    "    lower_bound = series.quantile(lower_quantile)\n",
    "    upper_bound = series.quantile(upper_quantile)\n",
    "    return series.clip(lower=lower_bound, upper=upper_bound)\n",
    "\n",
    "df['magnitudo'] = cap_outliers(df['magnitudo'])\n",
    "df['depth'] = cap_outliers(df['depth'])\n",
    "\n",
    "# 4. Create interaction features\n",
    "df['lat_lon_interaction'] = df['latitude'] * df['longitude'] / 10000  # Scaled down\n",
    "\n",
    "# 5. Create magnitude categories\n",
    "df['mag_low'] = (df['magnitudo'] < 2.0).astype(int)\n",
    "df['mag_medium'] = ((df['magnitudo'] >= 2.0) & (df['magnitudo'] < 4.0)).astype(int)\n",
    "df['mag_high'] = (df['magnitudo'] >= 4.0).astype(int)\n",
    "\n",
    "# 6. Add geographic region clustering\n",
    "def assign_region(lat, lon):\n",
    "    # Simplified region assignment based on latitude/longitude\n",
    "    if lat > 30 and lon > 120:  # Pacific Ring of Fire - Asia side\n",
    "        return 1\n",
    "    elif lat > 30 and lon < -100:  # Pacific Ring of Fire - Americas side\n",
    "        return 2\n",
    "    elif -30 < lat < 30:  # Equatorial regions\n",
    "        return 3\n",
    "    else:  # Other regions\n",
    "        return 4\n",
    "\n",
    "df['region'] = df.apply(lambda x: assign_region(x['latitude'], x['longitude']), axis=1)\n",
    "df = pd.get_dummies(df, columns=['region'], prefix='region')\n",
    "\n",
    "# Fill any remaining NaN values\n",
    "df.fillna(method='bfill', inplace=True)\n",
    "\n",
    "# Define features\n",
    "features = [\n",
    "    'latitude', 'longitude', 'depth',  \n",
    "    'month_sin', 'month_cos', 'day_sin', 'day_cos', 'hour_sin', 'hour_cos',\n",
    "    'rolling_mag_mean_30d', 'rolling_depth_mean_30d', 'rolling_quake_count_30d',\n",
    "    'rolling_mag_mean_7d', 'rolling_mag_mean_90d',\n",
    "    'lat_lon_interaction', 'mag_low', 'mag_medium', 'mag_high',\n",
    "    'region_1', 'region_2', 'region_3', 'region_4'\n",
    "]\n",
    "\n",
    "targets = ['latitude', 'longitude', 'magnitudo']\n",
    "\n",
    "# Extract features and targets\n",
    "X = df[features].values\n",
    "y = df[targets].values\n",
    "\n",
    "# Use time-based split for temporal data\n",
    "test_size = int(0.1 * len(X))\n",
    "val_size = int(0.1 * len(X))\n",
    "\n",
    "X_train = X[:-test_size-val_size]\n",
    "X_val = X[-test_size-val_size:-test_size]\n",
    "X_test = X[-test_size:]\n",
    "\n",
    "y_train = y[:-test_size-val_size]\n",
    "y_val = y[-test_size-val_size:-test_size]\n",
    "y_test = y[-test_size:]\n",
    "\n",
    "# Scale the features\n",
    "feature_scaler = RobustScaler()\n",
    "X_train_scaled = feature_scaler.fit_transform(X_train)\n",
    "X_val_scaled = feature_scaler.transform(X_val)\n",
    "X_test_scaled = feature_scaler.transform(X_test)\n",
    "\n",
    "# Random Forest doesn't require target scaling, but we'll scale for evaluation consistency\n",
    "target_scaler = StandardScaler()\n",
    "y_train_scaled = target_scaler.fit_transform(y_train)\n",
    "y_val_scaled = target_scaler.transform(y_val)\n",
    "y_test_scaled = target_scaler.transform(y_test)\n",
    "\n",
    "# Train Random Forest models for latitude and longitude\n",
    "models = []\n",
    "target_names = ['Latitude', 'Longitude', 'Magnitude']\n",
    "\n",
    "# Train only latitude and longitude models with Random Forest\n",
    "for i in range(2):  # Only for latitude and longitude (indices 0 and 1)\n",
    "    print(f\"\\nTraining Random Forest for {target_names[i]}...\")\n",
    "    \n",
    "    # Create and train the model\n",
    "    rf_model = RandomForestRegressor(\n",
    "        n_estimators=100,        # Number of trees\n",
    "        max_depth=15,            # Maximum depth of trees\n",
    "        min_samples_split=5,     # Minimum samples required to split\n",
    "        min_samples_leaf=2,      # Minimum samples required at leaf node\n",
    "        max_features='sqrt',     # Max features to consider for best split\n",
    "        n_jobs=-1,              # Use all available processors\n",
    "        random_state=42          # For reproducibility\n",
    "    )\n",
    "    \n",
    "    # Fit the model to the training data\n",
    "    rf_model.fit(X_train_scaled, y_train_scaled[:, i])\n",
    "    models.append(rf_model)\n",
    "    \n",
    "    # Make predictions on validation set to check performance\n",
    "    val_pred = rf_model.predict(X_val_scaled)\n",
    "    val_mse = mean_squared_error(y_val_scaled[:, i], val_pred)\n",
    "    val_rmse = np.sqrt(val_mse)\n",
    "    val_mae = mean_absolute_error(y_val_scaled[:, i], val_pred)\n",
    "    val_r2 = r2_score(y_val_scaled[:, i], val_pred)\n",
    "    \n",
    "    print(f\"Validation RMSE: {val_rmse:.4f}\")\n",
    "    print(f\"Validation MAE: {val_mae:.4f}\")\n",
    "    print(f\"Validation R²: {val_r2:.4f}\")\n",
    "\n",
    "#=== LSTM-Wavelet Model for Magnitude Prediction ===\n",
    "print(\"\\nTraining LSTM-Wavelet Model for Magnitude...\")\n",
    "\n",
    "# Create a time series representation for the magnitude data\n",
    "mag_data = df[['time', 'magnitudo']].copy()\n",
    "mag_data.set_index('time', inplace=True)\n",
    "# Resample to monthly data for wavelet analysis (adjust frequency if needed)\n",
    "monthly_mag = mag_data['magnitudo'].resample('M').mean().dropna()\n",
    "\n",
    "# Normalize magnitude data\n",
    "mag_scaler = MinMaxScaler()\n",
    "mag_data_scaled = mag_scaler.fit_transform(monthly_mag.values.reshape(-1, 1)).flatten()\n",
    "\n",
    "# Apply Discrete Wavelet Transform (DWT)\n",
    "wavename = 'db4'  # Daubechies wavelet\n",
    "coeffs = pywt.wavedec(mag_data_scaled, wavename, level=3)\n",
    "cA3, cD3, cD2, cD1 = coeffs  # Approximation and details\n",
    "\n",
    "# Reconstruct smoothed signal\n",
    "smoothed_data = pywt.waverec([cA3] + [None] * 3, wavename)[:len(mag_data_scaled)]\n",
    "\n",
    "# Prepare sequences for LSTM\n",
    "sequence_length = 30\n",
    "X_lstm, y_lstm = [], []\n",
    "for i in range(len(smoothed_data) - sequence_length):\n",
    "    X_lstm.append(smoothed_data[i:i+sequence_length])\n",
    "    y_lstm.append(smoothed_data[i+sequence_length])\n",
    "X_lstm, y_lstm = np.array(X_lstm), np.array(y_lstm)\n",
    "\n",
    "# Reshape input for LSTM\n",
    "X_lstm = X_lstm.reshape(X_lstm.shape[0], X_lstm.shape[1], 1)\n",
    "\n",
    "# Split into train and test sets\n",
    "split = int(0.8 * len(X_lstm))\n",
    "X_lstm_train, X_lstm_test = X_lstm[:split], X_lstm[split:]\n",
    "y_lstm_train, y_lstm_test = y_lstm[:split], y_lstm[split:]\n",
    "\n",
    "# Build LSTM model for magnitude prediction\n",
    "lstm_model = Sequential([\n",
    "    LSTM(100, return_sequences=True, input_shape=(sequence_length, 1)),\n",
    "    Dropout(0.3),\n",
    "    LSTM(100, return_sequences=False),\n",
    "    Dropout(0.3),\n",
    "    Dense(1)\n",
    "])\n",
    "\n",
    "# Compile and train the LSTM model\n",
    "lstm_model.compile(optimizer='adam', loss='mse')\n",
    "lstm_history = lstm_model.fit(\n",
    "    X_lstm_train, y_lstm_train, \n",
    "    epochs=100, \n",
    "    batch_size=32, \n",
    "    validation_data=(X_lstm_test, y_lstm_test),\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Evaluate LSTM-Wavelet model\n",
    "y_lstm_pred = lstm_model.predict(X_lstm_test)\n",
    "y_lstm_test_actual = y_lstm_test.reshape(-1, 1)\n",
    "\n",
    "# Scale back to original magnitude values\n",
    "y_lstm_test_denorm = mag_scaler.inverse_transform(y_lstm_test_actual)\n",
    "y_lstm_pred_denorm = mag_scaler.inverse_transform(y_lstm_pred)\n",
    "\n",
    "# Calculate magnitude prediction metrics\n",
    "lstm_mae = mean_absolute_error(y_lstm_test_denorm, y_lstm_pred_denorm)\n",
    "lstm_rmse = np.sqrt(mean_squared_error(y_lstm_test_denorm, y_lstm_pred_denorm))\n",
    "lstm_mape = np.mean(np.abs((y_lstm_test_denorm - y_lstm_pred_denorm) / np.maximum(0.0001, y_lstm_test_denorm))) * 100\n",
    "lstm_accuracy = 100 - lstm_mape  # Accuracy as (100 - MAPE)\n",
    "\n",
    "print(\"\\n=== LSTM-Wavelet Magnitude Model Performance ===\")\n",
    "print(f\"Mean Absolute Error (MAE): {lstm_mae:.4f}\")\n",
    "print(f\"Root Mean Squared Error (RMSE): {lstm_rmse:.4f}\")\n",
    "print(f\"Mean Absolute Percentage Error (MAPE): {lstm_mape:.2f}%\")\n",
    "print(f\"Forecast Accuracy: {lstm_accuracy:.2f}%\")\n",
    "\n",
    "# Append LSTM model as the magnitude predictor\n",
    "models.append(lstm_model)  # Note: This is the LSTM model, not a RandomForest\n",
    "\n",
    "# Evaluate Random Forest models on test set\n",
    "print(\"\\n=== Random Forest Models Performance (Latitude & Longitude) ===\")\n",
    "\n",
    "for i, (model, target_name) in enumerate(zip(models[:2], target_names[:2])):  # Only evaluate the RF models\n",
    "    # Make predictions\n",
    "    y_train_pred = model.predict(X_train_scaled)\n",
    "    y_test_pred = model.predict(X_test_scaled)\n",
    "    \n",
    "    # Convert predictions to original scale for a single target\n",
    "    y_train_true_single = y_train_scaled[:, i]\n",
    "    y_test_true_single = y_test_scaled[:, i]\n",
    "    \n",
    "    # Calculate metrics\n",
    "    train_mse = mean_squared_error(y_train_true_single, y_train_pred)\n",
    "    test_mse = mean_squared_error(y_test_true_single, y_test_pred)\n",
    "    train_rmse = np.sqrt(train_mse)\n",
    "    test_rmse = np.sqrt(test_mse)\n",
    "    train_mae = mean_absolute_error(y_train_true_single, y_train_pred)\n",
    "    test_mae = mean_absolute_error(y_test_true_single, y_test_pred)\n",
    "    train_r2 = r2_score(y_train_true_single, y_train_pred)\n",
    "    test_r2 = r2_score(y_test_true_single, y_test_pred)\n",
    "    \n",
    "    # Overfitting ratio\n",
    "    mse_overfitting_ratio = test_mse / train_mse if train_mse > 0 else float('inf')\n",
    "    \n",
    "    print(f\"\\n{target_name} Performance:\")\n",
    "    print(f\"  Train MSE: {train_mse:.4f}, Test MSE: {test_mse:.4f}\")\n",
    "    print(f\"  Train RMSE: {train_rmse:.4f}, Test RMSE: {test_rmse:.4f}\")\n",
    "    print(f\"  Train MAE: {train_mae:.4f}, Test MAE: {test_mae:.4f}\")\n",
    "    print(f\"  Train R²: {train_r2:.4f}, Test R²: {test_r2:.4f}\")\n",
    "    print(f\"  MSE Overfitting Ratio (Test/Train): {mse_overfitting_ratio:.2f}\")\n",
    "\n",
    "# Visualize feature importances for Random Forest models\n",
    "plt.figure(figsize=(14, 8))\n",
    "\n",
    "for i, (model, target_name) in enumerate(zip(models[:2], target_names[:2])):  # Only for RF models\n",
    "    plt.subplot(2, 1, i+1)\n",
    "    \n",
    "    # Get feature importances\n",
    "    importances = model.feature_importances_\n",
    "    indices = np.argsort(importances)\n",
    "    \n",
    "    plt.barh(range(len(indices)), importances[indices], color='skyblue')\n",
    "    plt.yticks(range(len(indices)), [features[i] for i in indices])\n",
    "    plt.title(f'Feature Importance for {target_name}')\n",
    "    plt.xlabel('Relative Importance')\n",
    "    plt.tight_layout()\n",
    "\n",
    "plt.savefig('random_forest_feature_importance.png')\n",
    "plt.close()\n",
    "\n",
    "# Visualize LSTM training history\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(lstm_history.history['loss'], label='Training Loss')\n",
    "plt.plot(lstm_history.history['val_loss'], label='Validation Loss')\n",
    "plt.title('LSTM-Wavelet Model Loss for Magnitude Prediction')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Mean Squared Error')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.savefig('lstm_wavelet_training_history.png')\n",
    "plt.close()\n",
    "\n",
    "# Visualize LSTM magnitude predictions\n",
    "plt.figure(figsize=(12, 6))\n",
    "# Plot only a subset for clarity\n",
    "n_points = min(100, len(y_lstm_test_denorm))\n",
    "plt.plot(y_lstm_test_denorm[-n_points:], label='Actual Magnitude', marker='o', linestyle='-', alpha=0.7)\n",
    "plt.plot(y_lstm_pred_denorm[-n_points:], label='Predicted Magnitude', marker='x', linestyle='-', alpha=0.7)\n",
    "plt.title('Magnitude Prediction: Actual vs. Predicted')\n",
    "plt.xlabel('Sample Index')\n",
    "plt.ylabel('Magnitude')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.savefig('lstm_wavelet_magnitude_predictions.png')\n",
    "plt.close()\n",
    "\n",
    "# Combined prediction function\n",
    "def predict_earthquake(lat_lon_features, historical_mag_sequence, models, feature_scaler, mag_scaler):\n",
    "    \"\"\"\n",
    "    Make predictions using both the Random Forest models (lat, lon) and LSTM-Wavelet model (magnitude)\n",
    "    \n",
    "    Parameters:\n",
    "    lat_lon_features: Numpy array of shape (n_samples, n_features) for lat/lon prediction\n",
    "    historical_mag_sequence: Sequence of magnitude values of length 'sequence_length' for LSTM\n",
    "    models: List of models [lat_rf_model, lon_rf_model, mag_lstm_model]\n",
    "    feature_scaler: Fitted scaler for RF input features\n",
    "    mag_scaler: Fitted scaler for magnitude\n",
    "    \n",
    "    Returns:\n",
    "    prediction: Dictionary with predicted latitude, longitude, and magnitude\n",
    "    \"\"\"\n",
    "    # Scale the input features for Random Forest\n",
    "    scaled_input = feature_scaler.transform(lat_lon_features)\n",
    "    \n",
    "    # Get lat/lon predictions from Random Forest\n",
    "    lat_pred = models[0].predict(scaled_input)\n",
    "    lon_pred = models[1].predict(scaled_input)\n",
    "    \n",
    "    # Prepare input for LSTM (magnitude prediction)\n",
    "    # Normalize the historical magnitudes\n",
    "    mag_sequence_scaled = mag_scaler.transform(historical_mag_sequence.reshape(-1, 1)).flatten()\n",
    "    \n",
    "    # Apply same wavelet transform as in training\n",
    "    wavename = 'db4'\n",
    "    coeffs = pywt.wavedec(mag_sequence_scaled, wavename, level=3)\n",
    "    cA3, cD3, cD2, cD1 = coeffs\n",
    "    smoothed_seq = pywt.waverec([cA3] + [None] * 3, wavename)[:len(mag_sequence_scaled)]\n",
    "    \n",
    "    # Reshape for LSTM input [batch, timesteps, features]\n",
    "    lstm_input = smoothed_seq.reshape(1, len(smoothed_seq), 1)\n",
    "    \n",
    "    # Predict magnitude\n",
    "    mag_pred_scaled = models[2].predict(lstm_input)\n",
    "    mag_pred = mag_scaler.inverse_transform(mag_pred_scaled)[0][0]\n",
    "    \n",
    "    return {\n",
    "        'latitude': lat_pred[0],\n",
    "        'longitude': lon_pred[0],\n",
    "        'magnitude': mag_pred\n",
    "    }\n",
    "\n",
    "print(\"\\nCombined earthquake prediction model trained successfully.\")\n",
    "print(\"Use the predict_earthquake() function to make new predictions.\")\n",
    "print(\"\\n=== Model Performance Summary ===\")\n",
    "print(\"Latitude & Longitude: Random Forest model\")\n",
    "print(\"Magnitude: LSTM model with Wavelet transform\")\n",
    "print(\"\\nThe combined approach addresses overfitting in magnitude prediction\")\n",
    "print(\"while maintaining high accuracy for location prediction.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4fd53996-1b6f-4777-9629-f0907fc7f99b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jidub\\AppData\\Local\\Temp\\ipykernel_22840\\1647483206.py:31: FutureWarning: DataFrame.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  df.fillna(method='ffill', inplace=True)\n",
      "C:\\Users\\jidub\\AppData\\Local\\Temp\\ipykernel_22840\\1647483206.py:85: FutureWarning: DataFrame.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  df.fillna(method='bfill', inplace=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Random Forest for Latitude...\n",
      "\n",
      "Training Gradient Boosting for Longitude...\n",
      "\n",
      "Latitude Validation Metrics:\n",
      "  Validation RMSE: 0.0241\n",
      "  Cross-Val RMSE: 0.0287\n",
      "  Validation MAE: 0.0125\n",
      "  Validation R²: 0.9993\n",
      "\n",
      "Longitude Validation Metrics:\n",
      "  Validation RMSE: 0.0050\n",
      "  Cross-Val RMSE: 0.0049\n",
      "  Validation MAE: 0.0034\n",
      "  Validation R²: 1.0000\n",
      "\n",
      "Training LSTM-Wavelet Model for Magnitude...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jidub\\AppData\\Local\\Temp\\ipykernel_22840\\1647483206.py:188: FutureWarning: 'M' is deprecated and will be removed in a future version, please use 'ME' instead.\n",
      "  monthly_mag = mag_data['magnitudo'].resample('M').mean().dropna()\n",
      "C:\\Users\\jidub\\anaconda3\\Lib\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 68ms/step - loss: 2.0416 - val_loss: 1.5397\n",
      "Epoch 2/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - loss: 1.4538 - val_loss: 1.1545\n",
      "Epoch 3/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 1.0860 - val_loss: 0.8607\n",
      "Epoch 4/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 0.8122 - val_loss: 0.6446\n",
      "Epoch 5/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 0.6091 - val_loss: 0.4825\n",
      "Epoch 6/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - loss: 0.4572 - val_loss: 0.3642\n",
      "Epoch 7/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 0.3482 - val_loss: 0.2770\n",
      "Epoch 8/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - loss: 0.2660 - val_loss: 0.2123\n",
      "Epoch 9/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - loss: 0.2069 - val_loss: 0.1634\n",
      "Epoch 10/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 0.1609 - val_loss: 0.1275\n",
      "Epoch 11/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - loss: 0.1258 - val_loss: 0.1003\n",
      "Epoch 12/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - loss: 0.1027 - val_loss: 0.0791\n",
      "Epoch 13/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 0.0801 - val_loss: 0.0641\n",
      "Epoch 14/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 0.0659 - val_loss: 0.0535\n",
      "Epoch 15/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 0.0552 - val_loss: 0.0431\n",
      "Epoch 16/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - loss: 0.0465 - val_loss: 0.0358\n",
      "Epoch 17/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - loss: 0.0395 - val_loss: 0.0302\n",
      "Epoch 18/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 0.0350 - val_loss: 0.0258\n",
      "Epoch 19/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 0.0300 - val_loss: 0.0250\n",
      "Epoch 20/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 0.0300 - val_loss: 0.0211\n",
      "Epoch 21/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - loss: 0.0268 - val_loss: 0.0180\n",
      "Epoch 22/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - loss: 0.0222 - val_loss: 0.0164\n",
      "Epoch 23/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 0.0227 - val_loss: 0.0155\n",
      "Epoch 24/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 0.0189 - val_loss: 0.0140\n",
      "Epoch 25/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - loss: 0.0184 - val_loss: 0.0135\n",
      "Epoch 26/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - loss: 0.0193 - val_loss: 0.0128\n",
      "Epoch 27/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - loss: 0.0173 - val_loss: 0.0118\n",
      "Epoch 28/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 0.0186 - val_loss: 0.0115\n",
      "Epoch 29/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - loss: 0.0199 - val_loss: 0.0107\n",
      "Epoch 30/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - loss: 0.0165 - val_loss: 0.0103\n",
      "Epoch 31/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - loss: 0.0144 - val_loss: 0.0095\n",
      "Epoch 32/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - loss: 0.0149 - val_loss: 0.0106\n",
      "Epoch 33/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 37ms/step - loss: 0.0157 - val_loss: 0.0108\n",
      "Epoch 34/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - loss: 0.0163 - val_loss: 0.0088\n",
      "Epoch 35/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - loss: 0.0127 - val_loss: 0.0089\n",
      "Epoch 36/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step - loss: 0.0145 - val_loss: 0.0085\n",
      "Epoch 37/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step - loss: 0.0132 - val_loss: 0.0086\n",
      "Epoch 38/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - loss: 0.0136 - val_loss: 0.0087\n",
      "Epoch 39/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - loss: 0.0135 - val_loss: 0.0084\n",
      "Epoch 40/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step - loss: 0.0158 - val_loss: 0.0080\n",
      "Epoch 41/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step - loss: 0.0132 - val_loss: 0.0082\n",
      "Epoch 42/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step - loss: 0.0131 - val_loss: 0.0074\n",
      "Epoch 43/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step - loss: 0.0138 - val_loss: 0.0071\n",
      "Epoch 44/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step - loss: 0.0122 - val_loss: 0.0075\n",
      "Epoch 45/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - loss: 0.0125 - val_loss: 0.0076\n",
      "Epoch 46/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - loss: 0.0133 - val_loss: 0.0075\n",
      "Epoch 47/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - loss: 0.0127 - val_loss: 0.0071\n",
      "Epoch 48/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - loss: 0.0114 - val_loss: 0.0068\n",
      "Epoch 49/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 0.0114 - val_loss: 0.0066\n",
      "Epoch 50/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 0.0106 - val_loss: 0.0070\n",
      "Epoch 51/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step - loss: 0.0124 - val_loss: 0.0072\n",
      "Epoch 52/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - loss: 0.0114 - val_loss: 0.0069\n",
      "Epoch 53/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step - loss: 0.0113 - val_loss: 0.0069\n",
      "Epoch 54/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 0.0105 - val_loss: 0.0067\n",
      "Epoch 55/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - loss: 0.0113 - val_loss: 0.0067\n",
      "Epoch 56/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 0.0116 - val_loss: 0.0067\n",
      "Epoch 57/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - loss: 0.0117 - val_loss: 0.0062\n",
      "Epoch 58/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step - loss: 0.0106 - val_loss: 0.0062\n",
      "Epoch 59/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - loss: 0.0111 - val_loss: 0.0071\n",
      "Epoch 60/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 0.0126 - val_loss: 0.0079\n",
      "Epoch 61/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 28ms/step - loss: 0.0112 - val_loss: 0.0057\n",
      "Epoch 62/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - loss: 0.0103 - val_loss: 0.0063\n",
      "Epoch 63/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - loss: 0.0112 - val_loss: 0.0063\n",
      "Epoch 64/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - loss: 0.0115 - val_loss: 0.0061\n",
      "Epoch 65/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - loss: 0.0108 - val_loss: 0.0056\n",
      "Epoch 66/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - loss: 0.0105 - val_loss: 0.0055\n",
      "Epoch 67/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 0.0101 - val_loss: 0.0057\n",
      "Epoch 68/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - loss: 0.0098 - val_loss: 0.0055\n",
      "Epoch 69/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - loss: 0.0103 - val_loss: 0.0055\n",
      "Epoch 70/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 0.0095 - val_loss: 0.0062\n",
      "Epoch 71/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step - loss: 0.0111 - val_loss: 0.0066\n",
      "Epoch 72/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step - loss: 0.0111 - val_loss: 0.0061\n",
      "Epoch 73/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step - loss: 0.0104 - val_loss: 0.0052\n",
      "Epoch 74/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step - loss: 0.0093 - val_loss: 0.0050\n",
      "Epoch 75/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step - loss: 0.0092 - val_loss: 0.0052\n",
      "Epoch 76/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step - loss: 0.0085 - val_loss: 0.0059\n",
      "Epoch 77/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 50ms/step - loss: 0.0121 - val_loss: 0.0067\n",
      "Epoch 78/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step - loss: 0.0125 - val_loss: 0.0060\n",
      "Epoch 79/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step - loss: 0.0120 - val_loss: 0.0060\n",
      "Epoch 80/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 48ms/step - loss: 0.0108 - val_loss: 0.0057\n",
      "Epoch 81/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - loss: 0.0095 - val_loss: 0.0055\n",
      "Epoch 82/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - loss: 0.0088 - val_loss: 0.0052\n",
      "Epoch 83/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - loss: 0.0103 - val_loss: 0.0054\n",
      "Epoch 84/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 0.0093 - val_loss: 0.0055\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 135ms/step\n",
      "\n",
      "=== LSTM-Wavelet Magnitude Model Performance ===\n",
      "Mean Absolute Error (MAE): 0.0453\n",
      "Root Mean Squared Error (RMSE): 0.0590\n",
      "Mean Absolute Percentage Error (MAPE): 2.86%\n",
      "Forecast Accuracy: 97.14%\n",
      "\n",
      "=== Models Performance Evaluation ===\n",
      "\n",
      "Latitude Performance:\n",
      "  Train MSE: 0.0007, Test MSE: 0.0008\n",
      "  Train RMSE: 0.0262, Test RMSE: 0.0279\n",
      "  Train MAE: 0.0119, Test MAE: 0.0133\n",
      "  Train R²: 0.9993, Test R²: 0.9993\n",
      "  MSE Overfitting Ratio (Test/Train): 1.13\n",
      "\n",
      "Longitude Performance:\n",
      "  Train MSE: 0.0000, Test MSE: 0.0000\n",
      "  Train RMSE: 0.0061, Test RMSE: 0.0057\n",
      "  Train MAE: 0.0040, Test MAE: 0.0041\n",
      "  Train R²: 1.0000, Test R²: 1.0000\n",
      "  MSE Overfitting Ratio (Test/Train): 0.89\n",
      "\n",
      "Combined earthquake prediction model trained successfully.\n",
      "Use the predict_earthquake() function to make new predictions.\n",
      "\n",
      "=== Model Performance Summary ===\n",
      "Latitude: Random Forest model\n",
      "Longitude: Gradient Boosting model (reduced overfitting)\n",
      "Magnitude: LSTM model with Wavelet transform\n",
      "\n",
      "The combined approach addresses overfitting in both longitude and magnitude prediction\n",
      "while maintaining high accuracy for overall earthquake prediction.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler, RobustScaler, MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.inspection import permutation_importance\n",
    "import pywt\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "# Load dataset\n",
    "# Note: You'll need to replace this path with the location of your earthquake dataset\n",
    "df = pd.read_csv(r\"C:\\Users\\jidub\\OneDrive\\Documents\\Timeseries\\project\\Eartquakes-1990-2023.csv\\Eartquakes-1990-2023.csv\")\n",
    "\n",
    "# Preprocess timestamp\n",
    "df['time'] = pd.to_datetime(df['time'], unit='ms')\n",
    "df = df.sort_values('time')  # Ensure data is chronologically ordered\n",
    "df['year'] = df['time'].dt.year\n",
    "df['month'] = df['time'].dt.month\n",
    "df['day'] = df['time'].dt.day\n",
    "df['hour'] = df['time'].dt.hour\n",
    "\n",
    "# Handle missing values\n",
    "df.fillna(method='ffill', inplace=True)\n",
    "\n",
    "# === Feature Engineering ===\n",
    "# 1. Add cyclical time features\n",
    "df['month_sin'] = np.sin(df['month'] * (2 * np.pi / 12))\n",
    "df['month_cos'] = np.cos(df['month'] * (2 * np.pi / 12))\n",
    "df['hour_sin'] = np.sin(df['hour'] * (2 * np.pi / 24))\n",
    "df['hour_cos'] = np.cos(df['hour'] * (2 * np.pi / 24))\n",
    "df['day_sin'] = np.sin(df['day'] * (2 * np.pi / 31))\n",
    "df['day_cos'] = np.cos(df['day'] * (2 * np.pi / 31))\n",
    "\n",
    "# 2. Rolling statistics\n",
    "for window in [7, 30, 90]:\n",
    "    df[f'rolling_mag_mean_{window}d'] = df['magnitudo'].rolling(window=window).mean()\n",
    "    df[f'rolling_depth_mean_{window}d'] = df['depth'].rolling(window=window).mean()\n",
    "    df[f'rolling_quake_count_{window}d'] = df['magnitudo'].rolling(window=window).count()\n",
    "\n",
    "# 3. Handle outliers\n",
    "def cap_outliers(series, lower_quantile=0.01, upper_quantile=0.99):\n",
    "    lower_bound = series.quantile(lower_quantile)\n",
    "    upper_bound = series.quantile(upper_quantile)\n",
    "    return series.clip(lower=lower_bound, upper=upper_bound)\n",
    "\n",
    "df['magnitudo'] = cap_outliers(df['magnitudo'])\n",
    "df['depth'] = cap_outliers(df['depth'])\n",
    "\n",
    "# 4. Create interaction features\n",
    "df['lat_lon_interaction'] = df['latitude'] * df['longitude'] / 10000  # Scaled down\n",
    "\n",
    "# 5. Create magnitude categories\n",
    "df['mag_low'] = (df['magnitudo'] < 2.0).astype(int)\n",
    "df['mag_medium'] = ((df['magnitudo'] >= 2.0) & (df['magnitudo'] < 4.0)).astype(int)\n",
    "df['mag_high'] = (df['magnitudo'] >= 4.0).astype(int)\n",
    "\n",
    "# 6. Add geographic region clustering\n",
    "def assign_region(lat, lon):\n",
    "    # Simplified region assignment based on latitude/longitude\n",
    "    if lat > 30 and lon > 120:  # Pacific Ring of Fire - Asia side\n",
    "        return 1\n",
    "    elif lat > 30 and lon < -100:  # Pacific Ring of Fire - Americas side\n",
    "        return 2\n",
    "    elif -30 < lat < 30:  # Equatorial regions\n",
    "        return 3\n",
    "    else:  # Other regions\n",
    "        return 4\n",
    "\n",
    "df['region'] = df.apply(lambda x: assign_region(x['latitude'], x['longitude']), axis=1)\n",
    "df = pd.get_dummies(df, columns=['region'], prefix='region')\n",
    "\n",
    "# 7. Add squared and cubed terms for longitude model (to capture non-linear relationships)\n",
    "df['longitude_squared'] = df['longitude'] ** 2\n",
    "df['depth_squared'] = df['depth'] ** 2\n",
    "\n",
    "# Fill any remaining NaN values\n",
    "df.fillna(method='bfill', inplace=True)\n",
    "\n",
    "# Define features\n",
    "features = [\n",
    "    'latitude', 'longitude', 'depth',  \n",
    "    'month_sin', 'month_cos', 'day_sin', 'day_cos', 'hour_sin', 'hour_cos',\n",
    "    'rolling_mag_mean_30d', 'rolling_depth_mean_30d', 'rolling_quake_count_30d',\n",
    "    'rolling_mag_mean_7d', 'rolling_mag_mean_90d',\n",
    "    'lat_lon_interaction', 'mag_low', 'mag_medium', 'mag_high',\n",
    "    'region_1', 'region_2', 'region_3', 'region_4',\n",
    "    'longitude_squared', 'depth_squared'  # Added polynomial features\n",
    "]\n",
    "\n",
    "targets = ['latitude', 'longitude', 'magnitudo']\n",
    "\n",
    "# Extract features and targets\n",
    "X = df[features].values\n",
    "y = df[targets].values\n",
    "\n",
    "# Use time-based split for temporal data\n",
    "test_size = int(0.1 * len(X))\n",
    "val_size = int(0.1 * len(X))\n",
    "\n",
    "X_train = X[:-test_size-val_size]\n",
    "X_val = X[-test_size-val_size:-test_size]\n",
    "X_test = X[-test_size:]\n",
    "\n",
    "y_train = y[:-test_size-val_size]\n",
    "y_val = y[-test_size-val_size:-test_size]\n",
    "y_test = y[-test_size:]\n",
    "\n",
    "# Scale the features\n",
    "feature_scaler = RobustScaler()\n",
    "X_train_scaled = feature_scaler.fit_transform(X_train)\n",
    "X_val_scaled = feature_scaler.transform(X_val)\n",
    "X_test_scaled = feature_scaler.transform(X_test)\n",
    "\n",
    "# Scale targets for evaluation consistency\n",
    "target_scaler = StandardScaler()\n",
    "y_train_scaled = target_scaler.fit_transform(y_train)\n",
    "y_val_scaled = target_scaler.transform(y_val)\n",
    "y_test_scaled = target_scaler.transform(y_test)\n",
    "\n",
    "# Train Random Forest for latitude (keep the original)\n",
    "models = []\n",
    "\n",
    "# Latitude model \n",
    "print(\"\\nTraining Random Forest for Latitude...\")\n",
    "lat_model = RandomForestRegressor(\n",
    "    n_estimators=100,\n",
    "    max_depth=15,\n",
    "    min_samples_split=5,\n",
    "    min_samples_leaf=2,\n",
    "    max_features='sqrt',\n",
    "    n_jobs=-1,\n",
    "    random_state=42\n",
    ")\n",
    "lat_model.fit(X_train_scaled, y_train_scaled[:, 0])\n",
    "models.append(lat_model)\n",
    "\n",
    "# Train Gradient Boosting for longitude to reduce overfitting\n",
    "print(\"\\nTraining Gradient Boosting for Longitude...\")\n",
    "lon_model = GradientBoostingRegressor(\n",
    "    n_estimators=100,\n",
    "    learning_rate=0.05,  # Lower learning rate to reduce overfitting\n",
    "    max_depth=6,         # Reduced depth to avoid overfitting\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    subsample=0.8,       # Use 80% of samples for each tree\n",
    "    max_features=0.7,    # Use 70% of features for each split\n",
    "    random_state=42\n",
    ")\n",
    "lon_model.fit(X_train_scaled, y_train_scaled[:, 1])\n",
    "models.append(lon_model)\n",
    "\n",
    "# Validate models\n",
    "target_names = ['Latitude', 'Longitude', 'Magnitude']\n",
    "for i, (model, target_name) in enumerate(zip(models, target_names[:2])):\n",
    "    # Validate with cross-validation\n",
    "    cv_scores = cross_val_score(model, X_val_scaled, y_val_scaled[:, i], \n",
    "                               cv=5, scoring='neg_mean_squared_error')\n",
    "    cv_rmse = np.sqrt(-cv_scores.mean())\n",
    "    \n",
    "    # Make predictions on validation set\n",
    "    val_pred = model.predict(X_val_scaled)\n",
    "    val_mse = mean_squared_error(y_val_scaled[:, i], val_pred)\n",
    "    val_rmse = np.sqrt(val_mse)\n",
    "    val_mae = mean_absolute_error(y_val_scaled[:, i], val_pred)\n",
    "    val_r2 = r2_score(y_val_scaled[:, i], val_pred)\n",
    "    \n",
    "    print(f\"\\n{target_name} Validation Metrics:\")\n",
    "    print(f\"  Validation RMSE: {val_rmse:.4f}\")\n",
    "    print(f\"  Cross-Val RMSE: {cv_rmse:.4f}\")\n",
    "    print(f\"  Validation MAE: {val_mae:.4f}\")\n",
    "    print(f\"  Validation R²: {val_r2:.4f}\")\n",
    "\n",
    "#=== LSTM-Wavelet Model for Magnitude Prediction ===\n",
    "print(\"\\nTraining LSTM-Wavelet Model for Magnitude...\")\n",
    "\n",
    "# Create a time series representation for the magnitude data\n",
    "mag_data = df[['time', 'magnitudo']].copy()\n",
    "mag_data.set_index('time', inplace=True)\n",
    "# Resample to monthly data for wavelet analysis\n",
    "monthly_mag = mag_data['magnitudo'].resample('M').mean().dropna()\n",
    "\n",
    "# Normalize magnitude data\n",
    "mag_scaler = MinMaxScaler()\n",
    "mag_data_scaled = mag_scaler.fit_transform(monthly_mag.values.reshape(-1, 1)).flatten()\n",
    "\n",
    "# Apply Discrete Wavelet Transform (DWT)\n",
    "wavename = 'db4'  # Daubechies wavelet\n",
    "coeffs = pywt.wavedec(mag_data_scaled, wavename, level=3)\n",
    "cA3, cD3, cD2, cD1 = coeffs  # Approximation and details\n",
    "\n",
    "# Reconstruct smoothed signal\n",
    "smoothed_data = pywt.waverec([cA3] + [None] * 3, wavename)[:len(mag_data_scaled)]\n",
    "\n",
    "# Prepare sequences for LSTM\n",
    "sequence_length = 30\n",
    "X_lstm, y_lstm = [], []\n",
    "for i in range(len(smoothed_data) - sequence_length):\n",
    "    X_lstm.append(smoothed_data[i:i+sequence_length])\n",
    "    y_lstm.append(smoothed_data[i+sequence_length])\n",
    "X_lstm, y_lstm = np.array(X_lstm), np.array(y_lstm)\n",
    "\n",
    "# Reshape input for LSTM\n",
    "X_lstm = X_lstm.reshape(X_lstm.shape[0], X_lstm.shape[1], 1)\n",
    "\n",
    "# Split into train and test sets\n",
    "split = int(0.8 * len(X_lstm))\n",
    "X_lstm_train, X_lstm_test = X_lstm[:split], X_lstm[split:]\n",
    "y_lstm_train, y_lstm_test = y_lstm[:split], y_lstm[split:]\n",
    "\n",
    "# Build LSTM model for magnitude prediction with regularization\n",
    "lstm_model = Sequential([\n",
    "    LSTM(100, return_sequences=True, input_shape=(sequence_length, 1), \n",
    "         recurrent_regularizer=tf.keras.regularizers.l2(0.01)),  # Added L2 regularization\n",
    "    Dropout(0.4),  # Increased dropout rate\n",
    "    LSTM(100, return_sequences=False, \n",
    "         recurrent_regularizer=tf.keras.regularizers.l2(0.01)),  # Added L2 regularization\n",
    "    Dropout(0.4),  # Increased dropout rate\n",
    "    Dense(1, kernel_regularizer=tf.keras.regularizers.l2(0.01))  # Added L2 regularization\n",
    "])\n",
    "\n",
    "# Compile and train the LSTM model\n",
    "lstm_model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.001), loss='mse')\n",
    "lstm_history = lstm_model.fit(\n",
    "    X_lstm_train, y_lstm_train, \n",
    "    epochs=100, \n",
    "    batch_size=32, \n",
    "    validation_data=(X_lstm_test, y_lstm_test),\n",
    "    callbacks=[tf.keras.callbacks.EarlyStopping(\n",
    "        monitor='val_loss', patience=10, restore_best_weights=True)],  # Added early stopping\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Evaluate LSTM-Wavelet model\n",
    "y_lstm_pred = lstm_model.predict(X_lstm_test)\n",
    "y_lstm_test_actual = y_lstm_test.reshape(-1, 1)\n",
    "\n",
    "# Scale back to original magnitude values\n",
    "y_lstm_test_denorm = mag_scaler.inverse_transform(y_lstm_test_actual)\n",
    "y_lstm_pred_denorm = mag_scaler.inverse_transform(y_lstm_pred)\n",
    "\n",
    "# Calculate magnitude prediction metrics\n",
    "lstm_mae = mean_absolute_error(y_lstm_test_denorm, y_lstm_pred_denorm)\n",
    "lstm_rmse = np.sqrt(mean_squared_error(y_lstm_test_denorm, y_lstm_pred_denorm))\n",
    "lstm_mape = np.mean(np.abs((y_lstm_test_denorm - y_lstm_pred_denorm) / np.maximum(0.0001, y_lstm_test_denorm))) * 100\n",
    "lstm_accuracy = 100 - lstm_mape  # Accuracy as (100 - MAPE)\n",
    "\n",
    "print(\"\\n=== LSTM-Wavelet Magnitude Model Performance ===\")\n",
    "print(f\"Mean Absolute Error (MAE): {lstm_mae:.4f}\")\n",
    "print(f\"Root Mean Squared Error (RMSE): {lstm_rmse:.4f}\")\n",
    "print(f\"Mean Absolute Percentage Error (MAPE): {lstm_mape:.2f}%\")\n",
    "print(f\"Forecast Accuracy: {lstm_accuracy:.2f}%\")\n",
    "\n",
    "# Append LSTM model as the magnitude predictor\n",
    "models.append(lstm_model)  # Note: This is the LSTM model, not a RandomForest\n",
    "\n",
    "# Evaluate models on test set\n",
    "print(\"\\n=== Models Performance Evaluation ===\")\n",
    "\n",
    "for i, (model, target_name) in enumerate(zip(models[:2], target_names[:2])):\n",
    "    # Make predictions\n",
    "    y_train_pred = model.predict(X_train_scaled)\n",
    "    y_test_pred = model.predict(X_test_scaled)\n",
    "    \n",
    "    # Convert predictions to original scale for a single target\n",
    "    y_train_true_single = y_train_scaled[:, i]\n",
    "    y_test_true_single = y_test_scaled[:, i]\n",
    "    \n",
    "    # Calculate metrics\n",
    "    train_mse = mean_squared_error(y_train_true_single, y_train_pred)\n",
    "    test_mse = mean_squared_error(y_test_true_single, y_test_pred)\n",
    "    train_rmse = np.sqrt(train_mse)\n",
    "    test_rmse = np.sqrt(test_mse)\n",
    "    train_mae = mean_absolute_error(y_train_true_single, y_train_pred)\n",
    "    test_mae = mean_absolute_error(y_test_true_single, y_test_pred)\n",
    "    train_r2 = r2_score(y_train_true_single, y_train_pred)\n",
    "    test_r2 = r2_score(y_test_true_single, y_test_pred)\n",
    "    \n",
    "    # Overfitting ratio\n",
    "    mse_overfitting_ratio = test_mse / train_mse if train_mse > 0 else float('inf')\n",
    "    \n",
    "    print(f\"\\n{target_name} Performance:\")\n",
    "    print(f\"  Train MSE: {train_mse:.4f}, Test MSE: {test_mse:.4f}\")\n",
    "    print(f\"  Train RMSE: {train_rmse:.4f}, Test RMSE: {test_rmse:.4f}\")\n",
    "    print(f\"  Train MAE: {train_mae:.4f}, Test MAE: {test_mae:.4f}\")\n",
    "    print(f\"  Train R²: {train_r2:.4f}, Test R²: {test_r2:.4f}\")\n",
    "    print(f\"  MSE Overfitting Ratio (Test/Train): {mse_overfitting_ratio:.2f}\")\n",
    "\n",
    "# Visualize feature importances for models\n",
    "plt.figure(figsize=(14, 8))\n",
    "\n",
    "# For Random Forest (latitude)\n",
    "plt.subplot(2, 1, 1)\n",
    "importances = models[0].feature_importances_\n",
    "indices = np.argsort(importances)\n",
    "plt.barh(range(len(indices)), importances[indices], color='skyblue')\n",
    "plt.yticks(range(len(indices)), [features[i] for i in indices])\n",
    "plt.title('Feature Importance for Latitude (Random Forest)')\n",
    "plt.xlabel('Relative Importance')\n",
    "\n",
    "# For Gradient Boosting (longitude)\n",
    "plt.subplot(2, 1, 2)\n",
    "if hasattr(models[1], 'feature_importances_'):  # GradientBoostingRegressor has this\n",
    "    importances = models[1].feature_importances_\n",
    "    indices = np.argsort(importances)\n",
    "    plt.barh(range(len(indices)), importances[indices], color='lightgreen')\n",
    "    plt.yticks(range(len(indices)), [features[i] for i in indices])\n",
    "    plt.title('Feature Importance for Longitude (Gradient Boosting)')\n",
    "    plt.xlabel('Relative Importance')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('model_feature_importance.png')\n",
    "plt.close()\n",
    "\n",
    "# Visualize LSTM training history\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(lstm_history.history['loss'], label='Training Loss')\n",
    "plt.plot(lstm_history.history['val_loss'], label='Validation Loss')\n",
    "plt.title('LSTM-Wavelet Model Loss for Magnitude Prediction')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Mean Squared Error')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.savefig('lstm_wavelet_training_history.png')\n",
    "plt.close()\n",
    "\n",
    "# Visualize predictions\n",
    "plt.figure(figsize=(15, 15))\n",
    "\n",
    "# Visualize latitude prediction\n",
    "plt.subplot(3, 1, 1)\n",
    "y_lat_pred = models[0].predict(X_test_scaled)\n",
    "test_indices = np.random.choice(range(len(y_test_scaled)), min(100, len(y_test_scaled)), replace=False)\n",
    "plt.plot(y_test_scaled[test_indices, 0], label='Actual', marker='o', linestyle='', alpha=0.7)\n",
    "plt.plot(y_lat_pred[test_indices], label='Predicted', marker='x', linestyle='', alpha=0.7)\n",
    "plt.title('Latitude - Actual vs Predicted')\n",
    "plt.xlabel('Sample Index')\n",
    "plt.ylabel('Latitude (scaled)')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "# Visualize longitude prediction\n",
    "plt.subplot(3, 1, 2)\n",
    "y_lon_pred = models[1].predict(X_test_scaled)\n",
    "plt.plot(y_test_scaled[test_indices, 1], label='Actual', marker='o', linestyle='', alpha=0.7)\n",
    "plt.plot(y_lon_pred[test_indices], label='Predicted', marker='x', linestyle='', alpha=0.7)\n",
    "plt.title('Longitude - Actual vs Predicted')\n",
    "plt.xlabel('Sample Index')\n",
    "plt.ylabel('Longitude (scaled)')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "# For magnitude, we use a subset of the LSTM test data\n",
    "plt.subplot(3, 1, 3)\n",
    "n_points = min(100, len(y_lstm_test_denorm))\n",
    "plt.plot(y_lstm_test_denorm[-n_points:], label='Actual', marker='o', linestyle='-', alpha=0.7)\n",
    "plt.plot(y_lstm_pred_denorm[-n_points:], label='Predicted', marker='x', linestyle='-', alpha=0.7)\n",
    "plt.title('Magnitude - Actual vs Predicted')\n",
    "plt.xlabel('Sample Index')\n",
    "plt.ylabel('Magnitude')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('model_predictions.png')\n",
    "plt.close()\n",
    "\n",
    "# Combined prediction function\n",
    "def predict_earthquake(lat_lon_features, historical_mag_sequence, models, feature_scaler, mag_scaler):\n",
    "    \"\"\"\n",
    "    Make predictions using Random Forest (latitude), Gradient Boosting (longitude) \n",
    "    and LSTM-Wavelet (magnitude) models\n",
    "    \n",
    "    Parameters:\n",
    "    lat_lon_features: Numpy array of shape (n_samples, n_features) for lat/lon prediction\n",
    "    historical_mag_sequence: Sequence of magnitude values of length 'sequence_length' for LSTM\n",
    "    models: List of models [lat_rf_model, lon_gbr_model, mag_lstm_model]\n",
    "    feature_scaler: Fitted scaler for input features\n",
    "    mag_scaler: Fitted scaler for magnitude\n",
    "    \n",
    "    Returns:\n",
    "    prediction: Dictionary with predicted latitude, longitude, and magnitude\n",
    "    \"\"\"\n",
    "    # Scale the input features\n",
    "    scaled_input = feature_scaler.transform(lat_lon_features)\n",
    "    \n",
    "    # Get lat/lon predictions\n",
    "    lat_pred = models[0].predict(scaled_input)\n",
    "    lon_pred = models[1].predict(scaled_input)\n",
    "    \n",
    "    # Prepare input for LSTM (magnitude prediction)\n",
    "    # Normalize the historical magnitudes\n",
    "    mag_sequence_scaled = mag_scaler.transform(historical_mag_sequence.reshape(-1, 1)).flatten()\n",
    "    \n",
    "    # Apply same wavelet transform as in training\n",
    "    wavename = 'db4'\n",
    "    coeffs = pywt.wavedec(mag_sequence_scaled, wavename, level=3)\n",
    "    cA3, cD3, cD2, cD1 = coeffs\n",
    "    smoothed_seq = pywt.waverec([cA3] + [None] * 3, wavename)[:len(mag_sequence_scaled)]\n",
    "    \n",
    "    # Reshape for LSTM input [batch, timesteps, features]\n",
    "    lstm_input = smoothed_seq.reshape(1, len(smoothed_seq), 1)\n",
    "    \n",
    "    # Predict magnitude\n",
    "    mag_pred_scaled = models[2].predict(lstm_input)\n",
    "    mag_pred = mag_scaler.inverse_transform(mag_pred_scaled)[0][0]\n",
    "    \n",
    "    return {\n",
    "        'latitude': lat_pred[0],\n",
    "        'longitude': lon_pred[0],\n",
    "        'magnitude': mag_pred\n",
    "    }\n",
    "\n",
    "print(\"\\nCombined earthquake prediction model trained successfully.\")\n",
    "print(\"Use the predict_earthquake() function to make new predictions.\")\n",
    "print(\"\\n=== Model Performance Summary ===\")\n",
    "print(\"Latitude: Random Forest model\")\n",
    "print(\"Longitude: Gradient Boosting model (reduced overfitting)\")\n",
    "print(\"Magnitude: LSTM model with Wavelet transform\")\n",
    "print(\"\\nThe combined approach addresses overfitting in both longitude and magnitude prediction\")\n",
    "print(\"while maintaining high accuracy for overall earthquake prediction.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
